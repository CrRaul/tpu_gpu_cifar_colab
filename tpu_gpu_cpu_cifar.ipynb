{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmJNyYYO0WHH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import applications, datasets, utils, Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "fksyrVFvrSlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)"
      ],
      "metadata": {
        "id": "K9ox-QccLZgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cars196_dataset(path_to_dataset):\n",
        "    annotations = scipy.io.loadmat(os.path.join(path_to_dataset, 'cars_annos.mat'))\n",
        "\n",
        "    annotations = annotations[\"annotations\"][0]\n",
        "\n",
        "    train_indices = np.where(annotations[\"test\"] == 0)[1]\n",
        "    test_indices = np.where(annotations[\"test\"] == 1)[1]\n",
        "\n",
        "    def load_data(indices):\n",
        "        num_samples = len(indices)\n",
        "        car_annotations = annotations[indices]\n",
        "        x = np.empty((num_samples, ), dtype=object)\n",
        "        y = np.empty((num_samples, ), dtype=int)\n",
        "\n",
        "        for i, annotation in enumerate(car_annotations):\n",
        "            img_path = os.path.join(path_to_dataset, annotation[0][0])\n",
        "            img = Image.open(img_path)\n",
        "            x[i] = np.array(img)\n",
        "            y[i] = int(annotation[5]) - 1\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    x_train, y_train = load_data(train_indices)\n",
        "    x_test, y_test = load_data(test_indices)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n"
      ],
      "metadata": {
        "id": "Ho-BMae28mqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmLEcB3ixyEQ"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, dataset_name, epochs, device):\n",
        "    if device not in ['CPU', 'GPU', 'TPU']:\n",
        "        print('Unknown device')\n",
        "        return\n",
        "    if device == 'CPU':\n",
        "        tf.device('/CPU:0')\n",
        "    elif device == 'GPU':\n",
        "        tf.device('/GPU:0')\n",
        "    elif device == 'TPU':\n",
        "        try:\n",
        "            tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "            strategy = tf.distribute.TPUStrategy(resolver)\n",
        "        except KeyError:\n",
        "            print('TPU not found')\n",
        "            return\n",
        "        else:\n",
        "            tf.device('/TPU:0')\n",
        "\n",
        "\n",
        "    if model_name == 'resnet':\n",
        "        base_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    elif model_name == 'vgg':\n",
        "        base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    elif model_name == 'mobilenet':\n",
        "        base_model = applications.MobileNet(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    else:\n",
        "        print('Unknown model')\n",
        "        return\n",
        "\n",
        "\n",
        "    path_to_cars196 = '/content/gdrive/MyDrive/cars196'\n",
        "    if dataset_name == 'cifar10':\n",
        "        (x, y), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "    elif dataset_name == 'cifar100':\n",
        "        (x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
        "    elif dataset_name == 'cars193':\n",
        "        (x, y), (x_test, y_test) = load_cars196_dataset(path_to_cars196)\n",
        "    else:\n",
        "        print('Unknown dataset')\n",
        "        return\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_val = x_val.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    y_train = utils.to_categorical(y_train)\n",
        "    y_val = utils.to_categorical(y_val)\n",
        "    y_test = utils.to_categorical(y_test)\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(y_train.shape[1], activation='softmax')(x)\n",
        "    model = Model(base_model.input, output)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=(x_val, y_val))\n",
        "\n",
        "    time_callback = TimeHistory()\n",
        "    history = model.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_data=(x_val, y_val), callbacks=[time_callback])\n",
        "\n",
        "    return history, time_callback, model, x_test, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_display_results(history, time_callback, model, x_test, y_test, model_name, dataset_name, epoch ):\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    y_test_pred = model.predict(x_test)\n",
        "    cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_test_pred, axis=1))\n",
        "\n",
        "    total_time = sum(time_callback.times)\n",
        "\n",
        "    with open(f'results{model_name}_{dataset_name}_{epoch}.txt', 'w') as f:\n",
        "        f.write('Rezultate antrenament:\\n')\n",
        "        f.write(f'Test Loss: {test_loss}\\n')\n",
        "        f.write(f'Test Accuracy: {test_accuracy}\\n')\n",
        "        f.write('Matricea de confuzie:\\n')\n",
        "        f.write(np.array2string(cm, separator=', '))\n",
        "        f.write('\\nTimp total: {}\\n'.format(total_time))\n",
        "        f.write('Timp/epocÄƒ:\\n')\n",
        "        for i, time in enumerate(time_callback.times):\n",
        "            f.write('Epoca {}: {}\\n'.format(i+1, time))\n",
        "\n",
        "    print(f'test loss: {test_loss}\\ntest_acc: {test_accuracy}\\nconf matr:\\n{cm}\\ntimp total:{total_time}')\n",
        "    for i, time in enumerate(time_callback.times):\n",
        "      print('Epoca {}: {}\\n'.format(i+1, time))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss evolution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy evolution')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig('evolution.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    #plt.savefig('confusion_matrix.png')\n"
      ],
      "metadata": {
        "id": "P888S2g3NBz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    devices = ['GPU', 'CPU']\n",
        "    dataset_names = ['cifar10', 'cifar100']\n",
        "    model_names = ['resnet', 'vgg', 'mobilenet']\n",
        "    epochs = [10, 20, 50, 100]\n",
        "\n",
        "    for epoch in epochs:\n",
        "        for dataset_name in dataset_names:\n",
        "            for model_name in model_names:\n",
        "                for device in devices:\n",
        "                    print(f'{device}_{dataset_name}_{model_name}_{epoch}\\n')\n",
        "                    history, time_callback, model, x_test, y_test = train_model(model_name, dataset_name, epoch, device)\n",
        "                    evaluate_and_display_results(history, time_callback, model, x_test, y_test, model_name, dataset_name, epoch)\n"
      ],
      "metadata": {
        "id": "bCvJ5VPmNElY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "A_PhvI43NMoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}